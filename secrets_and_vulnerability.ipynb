{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secrets And Vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evil-Eye Henry was a pirate back in the day. He was not a famous one but he sure accumulated a lot of wealth. He died unexpectedly and his treasure was never found. It was buried in a **secret** spot that he never told anoyone about.\n",
    "\n",
    "Recently you found an old book mentioning his name. One of his sailors mentioned the following:\n",
    "\n",
    ">_**Arrrgh!**_ \n",
    ">\n",
    ">_I am 100% sure that Evil-Eye Henry has buried his treasure in one of the following locations:_\n",
    ">\n",
    ">1. _Black Sand Haven_\n",
    ">2. _Dead Man's Isle_\n",
    ">3. _Isle Of Mermaids_\n",
    ">4. _Kraken Reef_\n",
    ">5. _Monkey Bay_\n",
    ">6. _Old Salt Cavern_\n",
    ">\n",
    ">_I am also 50% sure that it must have been either the Black Sand Haven or the Dead Man's Isle._\n",
    ">\n",
    "> _**Arrrgh!**_\n",
    "\n",
    "Before finding the book you had no idea where the treasure might be. But now you know something more. How vunerable has Evil-Eye Henry's **secret** become?\n",
    "\n",
    "But first let's import libqif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from qif import *\n",
    "except: # install qif if not available (for running in colab, etc)\n",
    "    import IPython; IPython.get_ipython().run_line_magic('pip', 'install qif')\n",
    "    from qif import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Probability distribution of $X$\n",
    "\n",
    "Let's call our secret $X$. It matches the $X$ pirates used to mark treasures on their maps. The possible values for $X$ are $\\{1, 2, 3, 4, 5, 6\\}$. Each number corresponds to a location in the order they appear above. The probability distribution of $X$ is given by\n",
    "\n",
    "$$\n",
    "\\pi = \\left( \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{8}, \\frac{1}{8}, \\frac{1}{8} \\right)\n",
    "$$\n",
    "\n",
    "Why is that? The sailor said that $X$ is either in location $1$ or $2$ with probability $50\\%$. So we could deduce that $p_X(1) = \\frac{1}{4}$ and $p_X(2) = \\frac{1}{4}$ because they add up to $\\frac{1}{2} = 50\\%$. And for the remaining locations uniformly distribute the other $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.25, 0.125, 0.125, 0.125, 0.125]\n"
     ]
    }
   ],
   "source": [
    "pi = [1/4, 1/4, 1/8, 1/8, 1/8, 1/8]\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes vulnerability\n",
    "\n",
    "One way to measure the vulnerability of a probability distribution is Bayes vulnerability. It is basically the answer to the question:\n",
    "\n",
    ">What is the probability of guessing $X$ correctly in one try?\n",
    "\n",
    "For our scenario if we had only **one try** to go and dig up a place in search of the treasure we would naturally pick the one with the highest probability. Which would be either $1$ or $2$. Either way we would succeed with probability $\\frac{1}{4}$.\n",
    "\n",
    "In our case, Bayes vulnerability is equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes vulnerability: 0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"Bayes vulnerability:\", measure.bayes_vuln.prior(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guessing entropy\n",
    "\n",
    "Imagine now that you decide to go and search all of the locations. Eventually you would find the true value of $X$. But a question naturally arises. Which locations should you visit first? \n",
    "\n",
    "The logical way of doing it would be to visit the ones with the highest probability first. So there is a higher probability of finding the treasure sooner. \n",
    "\n",
    "Guessing entropy is the average number of locations we would need to search in order to find the true value of $X$.\n",
    "\n",
    "In our case, guessing entropy is equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing entropy: 3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Guessing entropy:\", measure.guessing.prior(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon entropy\n",
    "\n",
    "Another way to measure the vulnerability of a probability distribution is Shannon entropy. To better understand it imagine having in front of you someone who knows the true value of $X$ and you ask them questions according to the following train of thought:\n",
    "\n",
    ">_$X \\in \\{ 1, 2 \\}$ with probability $\\frac{1}{2}$ and $X \\in \\{ 3, 4, 5, 6 \\}$ with probability $\\frac{1}{2}$. So I ask:_\n",
    ">\n",
    ">_Does $X$ belong to $\\{1, 2\\}$?_\n",
    ">\n",
    ">>_If the answer is yes then $X \\in \\{ 1, 2 \\}$._  \n",
    ">>\n",
    ">>_$X \\in \\{ 1 \\}$ with probability $\\frac{1}{4}$ and $X \\in \\{ 2 \\}$ with probability $\\frac{1}{4}$. So I ask:_\n",
    ">>\n",
    ">>_Does $X$ belong to $\\{1\\}$?_\n",
    ">>\n",
    ">>>_If the answer is yes then I have found the treasure! It is buried in location 1 and it took me 2 questions to reach to this conclusion!_ [This scenario happens with probability $p_X(1)$]\n",
    ">>>\n",
    ">>>_If the answer is no then $X \\in \\{ 2 \\}$ and again, I have found the treasure! It is buried in location 2 and it took me 2 questions to reach to this conclusion!_ [This scenario happens with probability $p_X(2)$]\n",
    ">>\n",
    ">>_If the answer is no then $X \\in \\{ 3, 4, 5, 6 \\}$._ \n",
    ">>\n",
    ">>_$X \\in \\{ 3, 4 \\}$ with probability $\\frac{1}{4}$ and $X \\in \\{ 5, 6 \\}$ with probability $\\frac{1}{4}$. So I ask:_\n",
    ">>\n",
    ">>_Does $X$ belong to $\\{3, 4\\}$?_\n",
    ">>\n",
    ">>>_If the answer is yes then bla bla bla..._\n",
    "\n",
    "Shannon's entropy is basically the average number of questions needed to completely reveal the secret. In general, questions are in the form of _\"Does $x$ belong to $S$?\"_.\n",
    "\n",
    "In our case, Shannon's entropy is equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon entropy: 2.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Shannon entropy:\", measure.shannon.prior(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom line\n",
    "\n",
    "So how vulnerable is Evil-Eye Henry's secret? The answer is, well, it depends!\n",
    "\n",
    "If the person seeking to reveal the secret takes only one guess based on the secret's probability distribution, then Bayes Vulnerability is the measure of vulnerability we should be using.\n",
    "\n",
    "If the person seeking to reveal the secret tries all possible values of $X$ in decreasing probability order, then Guessing Entropy is the right measure of vulnerability for this case.\n",
    "\n",
    "And if they can ask questions in the form of _\"Does $x$ belong to $S$?\"_, then Shannon Entropy is the right choice.\n",
    "\n",
    "In general, we need to have a general idea of the adversary who wants to reveal our secret. What is their goal and what methods they can use. Shannon entropy does not often represent operational scenarios. Bayes vulnerability might be a natural choice if we don't have a specific adversarial model in our mind. Or guessing entropy if the adversary has multiple guesses for $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep experimenting\n",
    "\n",
    "The **maximum** value of bayes vulnerability $V_b(\\pi)$ is $1$ and the **minimum** is $\\frac{1}{|X|}$, where $|X|$ is the number of possible values $X$ can take. \n",
    "\n",
    "For this experiment imagine there are only three possible locations for the secret location, so $|X| = 3$ and  $\\frac{1}{|X|} = 0.333$.\n",
    "\n",
    "Play around with `pi` below and try to find a distribution which achieves those maximum and minimum values of $V_b(\\pi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes vulnerability: 0.5\n"
     ]
    }
   ],
   "source": [
    "pi = [1/2, 1/4, 1/4] # Remember they must add up to 1\n",
    "print(\"Bayes vulnerability:\", measure.bayes_vuln.prior(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The **maximum** value of guessing entropy $G(\\pi)$ is $\\frac{|X|+1}{2}$ and the **minimum** is $1$. \n",
    "\n",
    "Again, consider there are three elements, so $\\frac{|X|+1}{2} = 2$.\n",
    "\n",
    "Play around with `pi` below and try to find a distribution which achieves those maximum and minimum values of $G(\\pi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing entropy: 1.75\n"
     ]
    }
   ],
   "source": [
    "pi = [1/2, 1/4, 1/4] # Remember they must add up to 1\n",
    "print(\"Guessing entropy:\", measure.guessing.prior(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The **maximum** value of guessing entropy $H(\\pi)$ is $\\log_2|X|$ and the **minimum** is $0$.\n",
    "\n",
    "Again, consider there are three elements, so $\\log_2|X| = 1.584$.\n",
    "\n",
    "Play around with `pi` below and try to find a distribution which achieves those maximum and minimum values of $H(\\pi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon entropy: 1.5\n"
     ]
    }
   ],
   "source": [
    "pi = [1/2, 1/4, 1/4] # Remember they must add up to 1\n",
    "print(\"Shannon entropy:\", measure.shannon.prior(pi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
