{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Three Prisoners Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three prisoners, Alice, Bob and Charlie are senteced to death, but one of them (uniformly chosen at random) is selected to be pardoned, so that just the two out of the three prisoners will be executed. The warden knows which one will be pardoned, but he is not allowed to tell the prisoners. Alice begs the warden to let her know the identity of one of the others who will be executed saying: \n",
    "\n",
    "> _\"If Bob is pardoned, say Charlie's name, and if Charlie is pardoned say Bob's. If I'm pardoned, chose randomly to name Bob or Charlie.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given the warden's answer, we are interested in answering two questions:\n",
    "1. What is the probability of correctly guessing the pardoned prisoner?\n",
    "2. Is the wardenâ€™s answer useful for Alice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel matrix \n",
    "\n",
    "Let's model this problem using a channel $W$ that takes an input $x$ (the prisoner to be pardoned) and produces an output $y$ (the warden's answer). You can think of $W$ as being the warden in our problem. The possible values for $x$ and $y$ are $A, B, C$ (short for Alice, Bob and Charlie). Notice that the warden will never say Alice's name, but still for the more general case we include it. $W$ is defined as\n",
    "\n",
    "$$ \n",
    "W = \\left( \\begin{array} {ccc}\n",
    "    p(y=A | x=A) & p(y=B | x=A) & p(y=C | x=A) \\\\\n",
    "    p(y=A | x=B) & p(y=B | x=B) & p(y=C | x=B) \\\\\n",
    "    p(y=A | x=C) & p(y=B | x=C) & p(y=C | x=C) \\\\\n",
    "\\end{array} \\right) \n",
    "$$\n",
    "\n",
    "or in our case\n",
    "\n",
    "$$ \n",
    "W = \\left( \\begin{array} {cc}\n",
    "    0 & \\frac{1}{2} & \\frac{1}{2} \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "$W$'s first row corresponds to $x=A$, meaning the scenario where Alice is chosen to be pardoned. In that case the channel's output (or the warden's saying) is not deterministic, but has some degree of randomness. More specifically the warden says Alice with probability $0$, Bob with probability $\\frac{1}{2}$ and Charlie with probability $\\frac{1}{2}$. \n",
    "\n",
    "$W$'s second row corresponds to $x=B$, meaning the scenario where Bob is chosen to be pardoned. In that case, there is only one possible output, and that is Charlie. In this case, $W$ (the warden) behaves deterministically and this can be seen because the second row of $W$ has $0$ everywhere, except for one specific output, which gets probability $1$. Same happens with the third row, which corresponds to $x=C$, meaning Charlie is chosen to be pardoned.\n",
    "\n",
    "Columns now, correspond to outputs of $W$. The first column to $y=A$, the second to $y=B$ and the third to $y=C$. Notice that the first column contains $0$ everywhere. That is because the warden never says Alice's name. Or in a more general way of saying this, $W$ never produces output $A$. \n",
    "\n",
    "Notice also, that each row of $W$ sums up to $1$. This happens because each row defines a probability distribution, which basically says how $W$ (the warden) behaves given a specific input $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define $W$ using python and libqif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from qif import *\n",
    "except: # install qif if not available (for running in colab, etc)\n",
    "    import IPython; IPython.get_ipython().run_line_magic('pip', 'install qif')\n",
    "    from qif import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([\n",
    "    # y=A  y=B  y=C\n",
    "    [   0, 1/2, 1/2],    # x=A\n",
    "    [   0,   0,   1],    # x=B\n",
    "    [   0,   1,   0],    # x=C\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to answer Question 1 using QIF terminology, we want to find the **posterior vulnerability** of $W$. That is, what is the probability of correctly guessing the secret $x$ after observing the channel's output $y$. Let's see how we can compute that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior distribution \n",
    "\n",
    "First of all we must define the distribution of $x$. It is also called *the prior distribution* $\\pi$. In our case that is the probability of each prisoner being pardoned. The problem states that it is uniform, so we have\n",
    "\n",
    "$$\n",
    "p(x=A) = \\frac{1}{3} \\\\\n",
    "p(x=B) = \\frac{1}{3} \\\\\n",
    "p(x=C) = \\frac{1}{3} \\\\\n",
    "$$\n",
    "\n",
    "or for short\n",
    "\n",
    "$$\n",
    "\\pi = (\\frac{1}{3},\\frac{1}{3}, \\frac{1}{3}) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Matrix\n",
    "\n",
    "Next, we compute $J$, which is defined as\n",
    "\n",
    "$$ \n",
    "J = \\left( \\begin{array} {ccc}\n",
    "    p(y=A \\cap x=A) & p(y=B \\cap x=A) & p(y=C \\cap x=A) \\\\\n",
    "    p(y=A \\cap x=B) & p(y=B \\cap x=B) & p(y=C \\cap x=B) \\\\\n",
    "    p(y=A \\cap x=C) & p(y=B \\cap x=C) & p(y=C \\cap x=C) \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "$J$ contains the joint probabilities for each combination of $x$ and $y$. For computing $J$, we use the rule $p(S \\cap T) = p(T) \\cdot p(S | T) $.\n",
    "\n",
    "$$ \n",
    "J = \\left( \\begin{array} {ccc}\n",
    "    p(x=A) \\cdot p(y=A | x=A) & p(x=A) \\cdot p(y=B | x=A) & p(x=A) \\cdot p(y=C | x=A) \\\\\n",
    "    p(x=B) \\cdot p(y=A | x=B) & p(x=B) \\cdot p(y=B | x=B) & p(x=B) \\cdot p(y=C | x=B) \\\\\n",
    "    p(x=C) \\cdot p(y=A | x=C) & p(x=C) \\cdot p(y=B | x=C) & p(x=C) \\cdot p(y=C | x=C) \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Notice that $J$ depends on the channel $W$, **but also** on the distribution $\\pi$ of $x$. Meaning it depends on each of the $p(x=A)$, $p(x=B)$, $p(x=C)$. Thus, if the pardoned prisoner were not chosen at random, we would have a different $J$.\n",
    "\n",
    "If we compute $J$ for our case, it becomes\n",
    "\n",
    "$$ \n",
    "J = \\left( \\begin{array} {cc}\n",
    "    0 & \\frac{1}{6} & \\frac{1}{6} \\\\\n",
    "    0 & 0 & \\frac{1}{3} \\\\\n",
    "    0 & \\frac{1}{3} & 0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
