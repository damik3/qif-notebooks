{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Three Prisoners Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three prisoners, Alice, Bob anc Charlie are senteced to death, but one of them (uniformly chosen at random) is selected to be pardoned, so that just the two out of the three prisoners will be executed. The warden knows which one will be pardoned, but he is not allowed to tell the prisoners. Alice begs the warden to let her know the identity of one of the others who will be executed saying: \n",
    "\n",
    "> _\"If Bob is pardoned, say Charlie's name, and if Charlie is pardoned say Bob's. If I'm pardoned, chose randomly to name Bob or Charlie.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given the warden's answer, we are interested in answering two questions:\n",
    "1. What is the probability of correctly guessing the pardoned prisoner?\n",
    "2. Is the wardenâ€™s answer useful for Alice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel matrix $C$\n",
    "\n",
    "Let's model this problem using a channel $C$ that takes an input $x$ (the prisoner to be pardoned) and produces an output $y$ (the warden's answer). The possible values for $x$ and $y$ are $A, B, C$ (short for Alice, Bob and Charlie). Notice that the warden will never say Alice's name, but still for the more general case we include it. $C$ is defined as\n",
    "\n",
    "$$ \n",
    "C = \\left( \\begin{array} {ccc}\n",
    "    p(y=A | x=A) & p(y=B | x=A) & p(y=C | x=A) \\\\\n",
    "    p(y=A | x=B) & p(y=B | x=B) & p(y=C | x=B) \\\\\n",
    "    p(y=A | x=C) & p(y=B | x=C) & p(y=C | x=C) \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "or in our case\n",
    "\n",
    "$$ \n",
    "C = \\left( \\begin{array} {cc}\n",
    "    0 & \\frac12 & \\frac12 \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define $C$ using python and libqif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from qif import *\n",
    "except: # install qif if not available (for running in colab, etc)\n",
    "    import IPython; IPython.get_ipython().run_line_magic('pip', 'install qif')\n",
    "    from qif import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    # y=A  y=B  y=C\n",
    "    [   0, 1/2, 1/2],    # x=A\n",
    "    [   0,   0,   1],    # x=B\n",
    "    [   0,   1,   0],    # x=C\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to answer Question 1 using QIF terminology, we want to find the **posterior vulnerability** of $C$. That is, what is the probability of correctly guessing the secret $x$ after observing the channel's output $y$. Let's see how we can compute that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior distribution $\\pi$\n",
    "\n",
    "First of all we must define the distribution of $x$. It is also called *the prior distribution* $\\pi$. In our case that is the probability of each prisoner being pardoned. The problem states that it is uniform, so we have\n",
    "\n",
    "$$\n",
    "p(x=A) = \\frac13 \\\\\n",
    "p(x=B) = \\frac13 \\\\\n",
    "p(x=C) = \\frac13 \\\\\n",
    "$$\n",
    "\n",
    "or for short\n",
    "\n",
    "$$\n",
    "\\pi = (\\frac13,\\frac13, \\frac13) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Matrix $J$\n",
    "\n",
    "Next, we compute $J$, which is defined as\n",
    "\n",
    "$$ \n",
    "J = \\left( \\begin{array} {ccc}\n",
    "    p(y=A \\cap x=A) & p(y=B \\cap x=A) & p(y=C \\cap x=A) \\\\\n",
    "    p(y=A \\cap x=B) & p(y=B \\cap x=B) & p(y=C \\cap x=B) \\\\\n",
    "    p(y=A \\cap x=C) & p(y=B \\cap x=C) & p(y=C \\cap x=C) \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "$J$ contains the joint probabilities for each combination of $x$ and $y$. For computing $J$, we use the rule $p(S \\cap T) = p(T) \\cdot p(S | T) $.\n",
    "\n",
    "$$ \n",
    "J = \\left( \\begin{array} {ccc}\n",
    "    p(x=A) \\cdot p(y=A | x=A) & p(x=A) \\cdot p(y=B | x=A) & p(x=A) \\cdot p(y=C | x=A) \\\\\n",
    "    p(x=B) \\cdot p(y=A | x=B) & p(x=B) \\cdot p(y=B | x=B) & p(x=B) \\cdot p(y=C | x=B) \\\\\n",
    "    p(x=C) \\cdot p(y=A | x=C) & p(x=C) \\cdot p(y=B | x=C) & p(x=C) \\cdot p(y=C | x=C) \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Notice that $J$ depends on the channel $C$, **but also** on the distribution $\\pi$ of $x$. Meaning it depends on each of the $p(x=A)$, $p(x=B)$, $p(x=C)$.\n",
    "\n",
    "If we compute J for our case, it becomes\n",
    "\n",
    "$$ \n",
    "J = \\left( \\begin{array} {cc}\n",
    "    0 & \\frac16 & \\frac16 \\\\\n",
    "    0 & 0 & \\frac13 \\\\\n",
    "    0 & \\frac13 & 0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
