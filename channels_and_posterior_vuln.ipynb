{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channels and Posterior Vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is known as _\"The Three Prisoners Problem\"_.\n",
    "\n",
    "Three prisoners, Alice, Bob and Charlie are senteced to death, but one of them (uniformly chosen at random) is selected to be pardoned, so that just the two out of the three prisoners will be executed. The warden knows which one will be pardoned, but he is not allowed to tell the prisoners. Alice begs the warden to let her know the identity of one of the others who will be executed saying: \n",
    "\n",
    "> _\"If Bob is pardoned, say Charlie's name, and if Charlie is pardoned say Bob's. If I'm pardoned, chose randomly to name Bob or Charlie.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in answering the following two questions:\n",
    "1. Given the warden's answer, what is the probability of correctly guessing the pardoned prisoner?\n",
    "2. Is the warden’s answer useful for Alice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel matrix \n",
    "\n",
    "Let's model this problem using a channel $W$ that takes a secret input $X$ (the prisoner to be pardoned) and produces an output $Y$ (the warden's answer). You can think of $W$ as being the warden in our problem. The possible values for $X$ and $Y$ are $A, B, C$ (short for Alice, Bob and Charlie). Notice that the warden will never say Alice's name, but still for the more general case we include it. \n",
    "\n",
    "$$ \n",
    "W = \\left( \\begin{array} {cc}\n",
    "    0 & \\frac{1}{2} & \\frac{1}{2} \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "$W$'s first row corresponds to $X=A$, meaning the scenario where Alice is chosen to be pardoned. In that case the channel's output (or the warden's saying) is not deterministic, but has some degree of randomness. More specifically the warden says Alice with probability $0$, and chooses uniformly between Bob and Charlie, that is with probability $\\frac{1}{2}$ each. \n",
    "\n",
    "$W$'s second row corresponds to $X=B$, meaning the scenario where Bob is chosen to be pardoned. In that case, there is only one possible output, and that is Charlie. In this case, $W$ (the warden) behaves deterministically and this can be seen because the second row of $W$ has $0$ everywhere, except for one specific output, which gets probability $1$. Same happens with the third row, which corresponds to $X=C$, meaning Charlie is chosen to be pardoned.\n",
    "\n",
    "Remember that each row of $W$ sums up to $1$. This happens because each row defines a probability distribution, which basically says how $W$ (the warden) behaves given a specific input $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define $W$ using python and libqif:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    from qif import *\n",
    "except: # install qif if not available (for running in colab, etc)\n",
    "    import IPython; IPython.get_ipython().run_line_magic('pip', 'install qif')\n",
    "    from qif import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([\n",
    "    # Y=A  Y=B  Y=C\n",
    "    [   0, 1/2, 1/2],    # X=A\n",
    "    [   0,   0,   1],    # X=B\n",
    "    [   0,   1,   0],    # X=C\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to answer Question 1 using QIF terminology, we want to find the **posterior vulnerability** of $W$. That is, what is the probability of correctly guessing the secret $X$ after observing the channel's output $Y$. Let's see how we can compute that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior distribution \n",
    "\n",
    "The problem's description clearly states that the prisoner to be pardoned is uniformly chosen at random so we have\n",
    "\n",
    "$$\n",
    "p_X(A) = p_X(B) = p_X(C) = \\frac{1}{3} \n",
    "$$\n",
    "\n",
    "or for short\n",
    "\n",
    "$$\n",
    "\\pi = \\left(\\frac{1}{3},\\frac{1}{3}, \\frac{1}{3}\\right) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define that in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "pi = probab.uniform(3)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Matrix\n",
    "\n",
    "The joint matrix $J$ contains the joint probabilities for each combination of $X$ and $Y$.\n",
    "$$ \n",
    "J = \\left( \\begin{array} {cc}\n",
    "    0 & \\frac{1}{6} & \\frac{1}{6} \\\\\n",
    "    0 & 0 & \\frac{1}{3} \\\\\n",
    "    0 & \\frac{1}{3} & 0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Notice that $J$ depends on the channel $W$, **but also** on the distribution $\\pi$ of $X$. Meaning that it depends on all of the $p_X(A)$, $p_X(B)$, $p_X(C)$. Thus, if the pardoned prisoner were not chosen at random (meaning $\\pi$ was different), then $J$ would be different.\n",
    "\n",
    "Remember that if we sum all of $J$'s elements they add up to 1. That is excpected because by the defitition of probabiliy, when we sum the probabilities of every possible outcume, they must add up to 1. And that is exactly what happens when we sum $J$'s elements.\n",
    "\n",
    "For computing it we use the rule $p_{Y,X}(y,x) = p_X(x) \\cdot p_{Y|X}(y|x) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Distributions\n",
    "But we're most interested in *what exactly* does the warden's saying ($W$'s output) tells us about $X$. The posteriors distribution matrix $P$ helps us with that. It basically tells us the updated probability for $X$ **given that** we've observed the value of $Y$.\n",
    "\n",
    "$$ \n",
    "P = \\left( \\begin{array} {cc}\n",
    "    0 & \\frac{1}{3} & \\frac{1}{3} \\\\\n",
    "    0 & 0 & \\frac{2}{3} \\\\\n",
    "    0 & \\frac{2}{3} & 0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "$P$'s first column gives us the probabilities of each prisoner being pardoned, **given that** the warden has said Alice's name. That is, the probabilities of $X$ being $A, B$ or $C$ **given that** $Y=A$. Here we have $0$ everywhere because $Y=A$ never happens. We could basically remove this column because it corresponds to an output of $Y$ which happens with a probability of $0$.\n",
    "\n",
    "$P$'s second column gives us the probabilities of $X$ being $A, B$ or $C$ **given that** $Y=B$. The same happens with the third column.\n",
    "\n",
    "Remember that each non zero column gives us a new probability distribution (also meaning that each column adds up to 1). That is, it tells us the **updated** probability  of each $X$ **after** observing $W$'s output.\n",
    "\n",
    "$P$ can be computed using the rules $p(y) = \\sum_{x \\in X} p_{X, Y}(x, y)$ and $p_{X|Y}(x|y) = \\frac{p_{X, Y}(x, y)}{p_Y(y)}$. Notice that we have already calculated all joint probabilities in $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       nan 0.33333333 0.33333333]\n",
      " [       nan 0.         0.66666667]\n",
      " [       nan 0.66666667 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "P = channel.posteriors(W, pi)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally solving the problem\n",
    "\n",
    "#### Question 2\n",
    "\n",
    "We are now ready to answer the two questions we set for ourselves. First we will answer the question number 2, which asks\n",
    "\n",
    ">Is the warden’s answer useful for Alice?\n",
    "\n",
    "Before hearing the warden's saying ($W$'s output), Alice knew that she had a $\\frac{1}{3}$ probability of surviving (becuase the pardoned prisoner were chosen uniformly at random).\n",
    "\n",
    "Now, using matrix $P$, we know that if the warden says Bob's name ($P$'s second column), then Alice has a $p(X=A | Y=B) = \\frac{1}{3}$ probability of surviving. \n",
    "\n",
    "With the same reasoning, we see that if the warden says Charlie's name ($P$'s third column), then Alice has a $p(X=A | Y=C) = \\frac{1}{3}$ probability of surviving.\n",
    "\n",
    "Notice that the warden never says Alice's name ($W$ never outputs $A$) and this can be seen by verifying that $p_Y(A) = 0$.\n",
    "\n",
    "So before the warden's answer, Alice survived with a probability of $\\frac{1}{3}$. After the warden's answer, no matter which name the warden says, Alice **again** survives with a probabiliy of $\\frac{1}{3}$.\n",
    "\n",
    "Looks like Alice should have picked her question more carefully, because *the warden's answer is never useful for Alice*.\n",
    "\n",
    "Note that this question could have been answered by using basic probability theory only. But since this is the original question of the problem and it fits quite well within the QIF train of thought, we thought it would be a good idea to include it.\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Now we will answer question 1, which asks\n",
    "\n",
    ">Given the warden's answer, what is the probability of correctly guessing the pardoned prisoner?\n",
    "\n",
    "Consider the case where the warden says Bob's name ($P$'s second column). Alice has a probability of $\\frac{1}{3}$ of being pardoned, Bob $0$ and Charlie $\\frac{2}{3}$. What would be your best guess for who has been pardoned? The logical answer would be Charlie, because he has the highest probability among the others **in that specific column**. So if the warden says Bob's name, you know that your best guess is Charlie and you have a probability of success $\\frac{2}{3}$.\n",
    "\n",
    "Using the same reasoning, we can see that if the warden says Charlie's name ($P$'s third column), your best guess would Bob because he has the highest probability **in that specific column**. So if the warden says Charlie's name, you know that your best guess is Bob and you have a probability of success $\\frac{2}{3}$.\n",
    "\n",
    "Before the warden's answer our best guess for who is the pardoned prisoner would be, well, anyone since they have the same probability of being pardoned. And our probability of success is $\\frac{1}{3}$. \n",
    "\n",
    ">This is the *prior vulnerability* of $W$, also called $V(\\pi)$. That is, the probability of correctly guessing the secret $W$ *before* observing the channel's output $Y$.\n",
    "\n",
    "But after receiving the warden's answer ($W$'s output) we see that we can make a guess with probability of success $\\frac{2}{3}$! In the first case we have to guess Charlie and in the second Bob. \n",
    "\n",
    ">This is the *posterior vulnerability* of $W$, also called $V(\\pi, C)$. That is, the probability of correctly guessing the secret $W$ *after* observing the channel's output $Y$.\n",
    "\n",
    "So to answer the question, *given the warden's answer, the probability of correctly guessing the pardoned prisoner is* $\\frac{2}{3}$.\n",
    "\n",
    "Alice's name never pops up so we don't have to deal with that. In fact we could use the same reasoning if we eliminated $P$'s first row, since it has $0$ everywhere.\n",
    "\n",
    "Generalizing this, we can compute the posterior vulnerability of any channel $W$ by first computing its posterior distribution matrix $P$, removing all $0$ columns, then finding for each column, its maximum element (which corresponds to the best guess when $W$'s output is the one corresponding to that column). In our case the maximum elements were $\\frac{2}{3}$ and $\\frac{2}{3}$. \n",
    "\n",
    "But we need a way to combine them and get one value representing the whole channel. We can do that by **weighing** each value with the probability of each column happening, that is by $p_Y(y)$. In our case the second column ($W$ outputs $B$) happens with probability of $p_Y(B) = \\frac{1}{2}$ and the third column ($W$ outputs $C$) happens with probability of $p_Y(C) = \\frac{1}{2}$. So if we combine them we get \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(\\pi, C) &= \\frac{1}{2} \\cdot \\frac{2}{3} + \\frac{1}{2} \\cdot \\frac{2}{3} \\\\\n",
    "V(\\pi, C) &= \\frac{2}{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is what we intuitively got without specifically using the general definition of posterior vulnerability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use libqif to get the channel's prior or posterior vulnerability with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Bayes vulnerability: 0.3333333333333333\n",
      "Posterior Bayes vulnerability: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Prior Bayes vulnerability:\", measure.bayes_vuln.prior(pi))\n",
    "print(\"Posterior Bayes vulnerability:\", measure.bayes_vuln.posterior(pi, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the best strategy for guessing the pardoned prisoner observing $W$'s output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best guessing strategy: [0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best guessing strategy:\", measure.bayes_vuln.strategy(pi, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 0 corresponds to Alice, 1 to bob and 2 to Charlie. The first element is for when $W$'s output is $A$, the second for when $W$'s output is $B$ and the third for $C$. The first element in our case does not have a meaning because $W$ never outputs $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare libqif's results with the ones we computed ourselves. Did we get everything right?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
