{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channels and Posterior Vulnerability (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part as well as the next one are gonna examine a couple of variations of the original _Three Prisoners_ problem. The first one is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if the warden uses a biased coin to answer the question?\n",
    "\n",
    "In the classic version of the problem Alice says to the warden\n",
    "\n",
    ">...If I'm pardoned, chose randomly to name Bob or Charlie.\n",
    "\n",
    "which has basically the same meaning as\n",
    "\n",
    ">...If I'm pardoned, flip a **fair coin** and if it lands on heads says Bob's name otherwise say Charlie's.\n",
    "\n",
    "But what would happen if the warden flipped a **biased coin** and made his choice according to that?\n",
    "\n",
    "A biased coin is characterized by a probability $p$. For example if $p = \\frac{2}{3}$ then it means that the coin lands on heads 2 out of 3 times and on tails 1 out of 3.\n",
    "\n",
    "If you haven't guessed it already, we're gonna experiment with a couple different values of $p$ and for that we're gonna define the `get_W(p)` and the `get_distribution(p)` function that are gonna speed things up a little. \n",
    "\n",
    "For this variation we are gonna keep assuming that **the pardoned prisoner is chosen uniformly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from qif import *\n",
    "except: # install qif if not available (for running in colab, etc)\n",
    "    import IPython; IPython.get_ipython().run_line_magic('pip', 'install qif')\n",
    "    from qif import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W(p):\n",
    "    C = np.array([\n",
    "        [0, p, 1-p],\n",
    "        [0, 0,   1],\n",
    "        [0, 1,   0],\n",
    "    ])\n",
    "    return C\n",
    "\n",
    "def get_distribution(p):\n",
    "    return np.array([p, (1-p)/2,(1-p)/2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_W(p)` takes as input a probability $p$ and creates the channel matrix $W$ (as discussed in the previous part) with the only difference that now the warden uses a biased coin characterized by $p$.\n",
    "\n",
    "`get_distribution(p)` takes as input a probability $p$ and creates a distribution $\\pi$ where:\n",
    " - if $p=1/3$, then you get the uniform distribution with each prisoner being chosen with a probability of $1/3$. \n",
    " - if $p > 1/3$, then the first prisoner (Alice) is more likely to be pardoned than any of the other two. \n",
    " - if $p < 1/3$, then the first prisoner (Alice) is the least likely of all to be pardoned.\n",
    "\n",
    "Now consider the following cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biased coin with $p = \\frac{2}{3}$\n",
    "\n",
    "Let's define $W$, and the prior distribution $\\pi$ in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      " [[0.         0.66666667 0.33333333]\n",
      " [0.         0.         1.        ]\n",
      " [0.         1.         0.        ]]\n",
      "pi\n",
      " [0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "W = get_W(2/3)\n",
    "print(\"W\\n\", W)\n",
    "pi = get_distribution(1/3)\n",
    "print(\"pi\\n\", pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how does this change our answer to the questions we're interested in? Let's quickly remember these questions.\n",
    "\n",
    ">1. Given the warden's answer, what is the probability of correctly guessing the pardoned prisoner?\n",
    ">2. Is the wardenâ€™s answer useful for Alice?\n",
    "\n",
    "For question 1, as we said in the previous part, the answer is basically given by the posterior vulnerability of $W$. Let's also print the prior vulnerability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Bayes vulnerability: 0.33333333333333337\n",
      "Posterior Bayes vulnerability = 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Prior Bayes vulnerability:\", measure.bayes_vuln.prior(pi))\n",
    "print(\"Posterior Bayes vulnerability =\", measure.bayes_vuln.posterior(pi, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is exactly the same as in the original problem, where the coin was not biased. Hmm, that looks a bit suspicious... \n",
    "\n",
    "For question 2, we're also gonna need the posteriors distribution matrix $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan 0.4  0.25]\n",
      " [ nan 0.   0.75]\n",
      " [ nan 0.6  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "P = channel.posteriors(W, pi)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the answer is basically given by $p(X = A | Y = y)$, by examining it for all values of $y$. There are two possible values for $y$ and these are $B$ and $C$. Now, $p(X = A | Y = B)$ corresponds to $P$'s first-row-second-column element which is $0.4$ and $p(X = A | Y = C)$ corresponds to $P$'s first-row-third-column element which is $0.25$. So from that we can deduct that if the warden says Bob's name then Alice has a $0.4$ chance of surviving but if the warden says Charlie's then it goes down to $0.25$. So here the warden's answer is somewhat useful for Alice, meaning that it updates her knowledge about the probability of her surviving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biased coin with $p = \\frac{3}{4}$\n",
    "\n",
    "For question 1, doing the same thing as before we see that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      " [[0.   0.75 0.25]\n",
      " [0.   0.   1.  ]\n",
      " [0.   1.   0.  ]] \n",
      "\n",
      "Prior Bayes vulnerability: 0.33333333333333337\n",
      "Posterior Bayes vulnerability = 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "W = get_W(3/4)\n",
    "print(\"W\\n\", W, \"\\n\")\n",
    "print(\"Prior Bayes vulnerability:\", measure.bayes_vuln.prior(pi))\n",
    "print(\"Posterior Bayes vulnerability =\", measure.bayes_vuln.posterior(pi, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that again, the posterior vunlerability of $W$ is the same. This implies that possibly, no matter the value of $p$, $V(\\pi, W)$ stays the same. This indeed can be experimentally verified by trying different values of $p$ and using python to plot the results. But before that, take a few moments to try yourself a few different values for the p parameter of `get_W(p)` in the cell above and rerunning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ps = np.linspace(0, 1, 100)\n",
    "plt.plot(ps, [measure.bayes_vuln.posterior(get_distribution(1/3), get_W(p)) for p in ps])\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Posterior vulerability on uniform prior')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we clearly see that $V(\\pi, W)$ indeed stays the same for all possible values of $p$. Which means that no matter what $p$ the warden uses to determine his answer, **our best guess** will always succeed with probability $\\frac{2}{3}$. We could say that all $W$ defined this way, have the same vulenrability against someone who tries to guess the pardoned prisoner in one try.\n",
    "\n",
    "To get a better insight of why this is happening take a look at the following piece of code. For each `p` from $0$ to $1$, it prints the `p` parameter itself, then the distribution $p_Y(y)$ for $y=B$ or $C$ (remember that Alice's name never pops up meaning $p_Y(A) = 0$, thus $y=A$ is omitted) and then array of the posterior distributions. Its output is in the form of\n",
    "\n",
    "```\n",
    "-------------------------------\n",
    "|    p(Y=B)         p(Y=C)    |\n",
    "-------------------------------\n",
    "| p(X=A | Y=B)   p(X=A | Y=C) |\n",
    "| p(X=B | Y=B)   p(X=B | Y=C) |\n",
    "| p(X=C | Y=B)   p(X=C | Y=C) |\n",
    "-------------------------------\n",
    "```\n",
    "\n",
    "It also marks for each column its maximum element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "p=0/10\n",
      "-------------------\n",
      "|    0.33    0.67 |\n",
      "-------------------\n",
      "|    0.00    0.50 |\n",
      "|    0.00 -->0.50 |\n",
      "| -->1.00    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=1/10\n",
      "-------------------\n",
      "|    0.37    0.63 |\n",
      "-------------------\n",
      "|    0.09    0.47 |\n",
      "|    0.00 -->0.53 |\n",
      "| -->0.91    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=2/10\n",
      "-------------------\n",
      "|    0.40    0.60 |\n",
      "-------------------\n",
      "|    0.17    0.44 |\n",
      "|    0.00 -->0.56 |\n",
      "| -->0.83    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=3/10\n",
      "-------------------\n",
      "|    0.43    0.57 |\n",
      "-------------------\n",
      "|    0.23    0.41 |\n",
      "|    0.00 -->0.59 |\n",
      "| -->0.77    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=4/10\n",
      "-------------------\n",
      "|    0.47    0.53 |\n",
      "-------------------\n",
      "|    0.29    0.38 |\n",
      "|    0.00 -->0.63 |\n",
      "| -->0.71    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=5/10\n",
      "-------------------\n",
      "|    0.50    0.50 |\n",
      "-------------------\n",
      "|    0.33    0.33 |\n",
      "|    0.00 -->0.67 |\n",
      "| -->0.67    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=6/10\n",
      "-------------------\n",
      "|    0.53    0.47 |\n",
      "-------------------\n",
      "|    0.38    0.29 |\n",
      "|    0.00 -->0.71 |\n",
      "| -->0.63    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=7/10\n",
      "-------------------\n",
      "|    0.57    0.43 |\n",
      "-------------------\n",
      "|    0.41    0.23 |\n",
      "|    0.00 -->0.77 |\n",
      "| -->0.59    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=8/10\n",
      "-------------------\n",
      "|    0.60    0.40 |\n",
      "-------------------\n",
      "|    0.44    0.17 |\n",
      "|    0.00 -->0.83 |\n",
      "| -->0.56    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=9/10\n",
      "-------------------\n",
      "|    0.63    0.37 |\n",
      "-------------------\n",
      "|    0.47    0.09 |\n",
      "|    0.00 -->0.91 |\n",
      "| -->0.53    0.00 |\n",
      "-------------------\n",
      "\n",
      "p=10/10\n",
      "-------------------\n",
      "|    0.67    0.33 |\n",
      "-------------------\n",
      "|    0.50    0.00 |\n",
      "|    0.00 -->1.00 |\n",
      "| -->0.50    0.00 |\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "from print_hyper import print_hyper\n",
    "\n",
    "for k in range(11):\n",
    "        print(\"\\np=\", k, \"/10\", sep='')\n",
    "        print_hyper(get_W(k/10), get_distribution(1/3), highlight_maxima=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the maximum elemnts of each column stay at the same positions as `p` increases. It means that upon observing a specific output $y$, our best guess always stays the same regardless of the distribution of $X$.\n",
    "\n",
    "Remember that the posterior vulnerability in our case is calculated by \n",
    "\n",
    "$$\n",
    "V(\\pi, W) = p_Y(B)\\cdot max_B +  p_Y(C)\\cdot max_C\n",
    "$$\n",
    "\n",
    "where $max_B$ and $max_C$ are the maximum elements of the first and second columns respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
